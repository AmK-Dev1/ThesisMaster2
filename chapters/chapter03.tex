\chapter{Driver Monitoring}

\emph{The chapter begins with a brief discussion of signs of drowsiness and available methods for detecting them. The chapter continues with our optimization techniques to improve the performance of these methods in terms of speed, detection rate, and detection accuracy under non-ideal lighting conditions and for noisy images.}


\section{Introduction}
A driver-monitoring system is an advanced safety feature that track driver drowsiness or distraction, and to issue a warning or alert to get the driver’s attention back to the task of driving.

Driver-monitoring systems typically use sensors to collect data about the driver and pass these data to a software. The program can then tell whether the driver is blinking more than normal, if his eyes are narrowing or shutting, and if his head is tilted at an unusual angle. It can also tell whether the motorist is gazing ahead at the road, and whether he or she is paying attention or merely staring absently.

\section{Driver Monitoring Technologies}
According to \parencite{DrowsinessDetectionSystem} there are many approaches and measurement technologies to predict driver's behaviors. The most commonly used measurement can be categorized upon the monitoring instrument into : 
\begin{itemize}
	\item Video-based sensors
	\item Physiological signals sensors 
\end{itemize}

\begin{figure}[!h]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{cmsensors}  
  \caption{Video-based sensors (used in this thesis)}
  \label{fig:CamSensors}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{phsensors}  
  \caption{physiological signals sensors}
  \label{fig:PhSensors}
\end{subfigure}
\caption{Illustration of different Driver monitoring approches}
\label{fig:DifrentMonitoringApproches}
\end{figure}


\subsection{Physiological Signals Sensors}
Physiological signals of the driver are the most accurate solutions, they can be used to measure his vigilance level since these signals originated from human organs such as the brain, eyes, muscles, and heart that can indicate the fatigue and alertness level in real-time as depicted in Figure \ref{fig:PhSensors}. Physiological measures can be recorded from different organs that show a visible correlation with the wakefulness/drowsiness state of a person. This includes \parencite{DrowsinessDetectionSystem}:

	\begin{itemize}
		\item \textbf{Brain activity}, which can be captured by electroencephalography (EEG) or Near Infrared Spectroscopy
(NIRS).
		\item \textbf{Cardiac activity}, monitored through electrocardiography (ECG) and Blood Pressure signals.
		\item \textbf{Ocular activity}, measured by electrooculography (EOG)
	\end{itemize}

\subsection{Video-based sensors}
To determine the alertness/drowsiness level of the driver as illustrated in Figure \ref{fig:CamSensors}. The behavior of the driver is mainly monitored through a camera and thus this approach is known as video-based measure. Visible symptoms of fatigue and sleepiness can be observed when the driver becomes drowsy through measuring its abnormal behaviors. Research on fatigue and drowsiness detection using driver behavioral monitoring focused on three main measures: \emph{Eyes state}, \emph{Face expression}, and \emph{Head position}.

\subsection{Evaluation}
\emph{The following evaluation and ranking are based on our online search for driver monitoring technologies and we believe that it is not the only evaluation method.}

According to \parencite{DrowsinessDetectionSystem}, physiological sensors make it possible to alert drivers at earlier stages of drowsiness and thereby prevent many drastic accidents \parencite{EvaluationOfTechnologies}. Because physiological measurements are less affected by ambient and road conditions, they have been proved to be dependable and accurate and thus may have fewer false positives \parencite{Zilberg2007MethodologyAI}.

On the other hand, video sensors technology is user-friendly and can be mounted comfortably in various areas inside a vehicle also, it has the lowest cost. The common limitation is lighting conditions.

\begin{table}[h!]
\centering

\begin{tabular}{ |c c c| } 
\hline
Technology & Video-Based Sensors & Physiological Signals Sensors \\
\hline
Coast & ++ & +\\
Ease of Use & +++ & + \\
Intrusiveness & + & +++ \\
Accuracy & ++ & +++\\
\hline
\end{tabular}
\caption{shows the evaluation results of the two above described Driver Monitoring Technologies. The (+) symbol represents the rating level}
\label{table:DriverMonitoringTEchnologiesEvaluationResult}
\end{table}



\section{Driver Drowsiness}
Feeling sleepy or tired during the day is commonly called drowsiness. Drowsiness can lead to additional symptoms, such as forgetting or falling asleep at inappropriate times, especially in the case of driving because it leads to a car accident.
Drowsiness in general is accompanied by warning signs that differ from one person to another, such as yawning\footnote{yawning is a response to fatigue,  it is characterized by opening up of mouth which is accompanied by a long inspiration, with a brief interruption of ventilation and followed by a short expiration.} or blinking frequently, nodding\footnote{nodding also is a response to fatigue, it is characterized by lower or raising the head slowly and briefly}, drifting off the track, and the most critical sign of drowsiness is closed eyes.\parencite{DrowsinessSigns}

\section{Driver Inattention}
When a driver participates in a secondary activity that interferes with the primary duty of operating a vehicle, this is known as driver inattention or distraction. Drivers can be distracted in many ways by things inside or outside of the vehicle. There are three categories in which driver distraction \parencite{DriverInnatention}:


\begin{enumerate}
\item \textbf{Visual} : taking your eyes off the road, such as :
						
		\begin{itemize}						
			\item Texting
			\item Other distractions outside the vehicle
		\end{itemize}
																			
					
\item \textbf{Physical} : taking your hands off the steering wheel, like :
		
		\begin{itemize}						
			\item Adjusting vehicle controls, such as the air conditioner
			\item Eating or drinking
		\end{itemize}

\item \textbf{Cognitive} : taking your attention away from the driving task. Cognitive distraction usually accompanies physical and visual distractions.
\end{enumerate}

\begin{figure}[!h]
\centering
\includegraphics[width=.99\textwidth]{distracteddrivers}
\caption{Distracted Driver Sceenes}
\label{fig:DDS}
\end{figure}

The head posture is a typical symptom of driver inattention when the driver looks in a direction other than the road is extremely dangerous, see Figure\ref{fig:DDS}.


For the application phase, the driver’s surveillance camera is mounted in front of the driver’s face it can be placed in the areas of the steering wheel or it can be hung on the rearview mirror. In the next sections, we discuss methods for detecting driver’s drowsiness and distraction using computer vision techniques discussed in chapter 2 with other algorithms that aim to improve performance and accuracy (ACC).




\section{method 1 : Haar Cascades}
\emph{Viola-Jones facial detection technique, commonly known as Haar Cascades. This work was done well before the beginning of the era of deep learning. But it’s a great job in comparison with powerful models that can be built with modern deep learning techniques, especially in terms of speed. The algorithm is still used almost everywhere.}\\

Haar Cascades in general is an object detection algorithm uses haar features. Haar Features were not only used to detect faces, but also for eyes, lips, license number plates, etc. \parencite{HaarFeaturesUses}.\\

For a good detection rate, we need a strong classifier that is trained in using a large set of \emph{positive}\footnote{Positive data points are areas that include a face} and \emph{negative}\footnote{Negative data points are examples of regions that do not contain a face} samples of a face, however, \emph{OpenCV}\footnote{Open Source Computer Vision is a programming library that focuses on real-time computer vision. It was created by Intel and then sponsored by Willow Garage and Itseez.} can perform face detection out-of-the-box using a pre-trained Haar cascade that is mean OpenCV's Haar cascade has already picked the best haar-like features for face detection, eyes detection, etc.\\

This eliminates the need for us to give our own positive and negative samples, train our own classifier, or worry about fine-tuning the parameters. Instead, we need to focus on improving speed, and accuracy and finding solutions for challenging conditions \parencite{PyImageSearchHaarCascades} \parencite{OpenCvHaarCascades} \parencite{HaarFeaturesUses}.

\section{Implementation}

In the application of this method, we use Haar-like detectors provided by \emph{OpenCv}, a haar-like detector takes as arguments \parencite{OpenCvHaarCascades}:

\begin{itemize}

	\item \textbf{Image}: matrix of the type \emph{CV\_8U} \footnote{CV\_8U is unsigned 8bit pixel, a pixel can have values 0-255, this is the normal range for most image and video formats.} containing an gray-scale image where objects are detected.

	\item \textbf{ScaleFactor}: parameter specifying how much the image size is reduced at each image scale.

	\item \textbf{MinNeighbors}: parameter specifying how many neighbors each candidate rectangle should have to retain it.

	\item \textbf{MinSize}: minimum possible object size. Objects smaller than this are ignored.
	
	\item \textbf{MaxSize}: maximum possible object size. Objects larger than this are ignored.

\end{itemize}

By default a haar-like detector returns 4 values \parencite{OpenCvHaarCascades} x-coordinate, y-coordinate, width(w), height(h) of the detected target object, these 4 values represent 2 spatial points $(x , y)$ and $(\quad x + w ,\quad y + h) $ of the rectangle that contains the object see Figure \ref{fig:bbprincipe}. 

\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{bbprincipe}
\caption{Illustration of Bounding Box in object detection using haar-like detector}
\label{fig:bbprincipe}
\end{figure}
 
Taking the Figure \ref{fig:bbprincipe} as an example. After a successful object detection, a haar-like detector returns :  $(x,\quad y, \quad w, \quad h) = (20, \quad 40, \quad 40, \quad 40)$ respectively.So, $p1 = (20, \quad 40)$ and $p2 = (20 + 40 , 40 + 40 ) = (60 ,\quad 80)$. However, these bounding boxes are useless in the case of driver monitoring, the most important thing is to alert the driver in real-time when he is asleep or distracted.\\

In the application stage of this method to serve as a driver monitoring technique. we used two Haar-like detectors, the first one is a face detector to check whether a given image contains a face or not, In other words, the face detector can check whether the driver is focusing on the road or distracted. Assuming the camera is in front of the driver’s face, if the face detector finds a face on a given image, This means that the driver looks forward and if the detector cannot find a face that means that the driver is distracted by looking in a direction other than the road, which is a dangerous situation, especially when driving fast.\\

The second Haar-like detector is an eye detector, which can find an "open" eye in a given image. Using this condition, the eye detector can check if the driver’s eyes are open or closed, simply by performing an eye detection if the detector finds one or both eyes that means the driver is focusing on the road if not, This means that one or both eyes of the driver are closed and this is the critical sign of the driver's drowsiness which requires an alert to wake up the driver.\\
 
As this method works with images, and the camera feeds the system with a video stream which is a sequence of images we processed the video frame by frame, and for each frame, we performed haar cascades see Figure\ref{fig:Initfchc}.

\begin{figure}[!h]
\centering
\includegraphics[width=.7\textwidth]{Init haarcascades}
\caption{Initial flowchart for driver monitoring with haarcascades}
\label{fig:Initfchc}
\end{figure}

\newpage

 The first implementation of this method was done with the following algorithm under different lighting conditions :  
 
 
\begin{algorithm}[!h]
  \caption{first algorithm for face/eyes detection using Haar cascades}
  \begin{itemize}
  	\item Load haar-like detectors provided by OpenCv
  	\item Foreach frame from video input :	
  	\begin{enumerate}	
		\item Change color space from RGB to Gray	
		\item Face detection with following parameters : 
		      \begin{itemize}
		      	\item \textbf{Image} : frame
		      	\item \textbf{ScaleFactor} : 1.1
		      	\item \textbf{MinNeighbors} :  2
		      \end{itemize}	     
		\item Eye detection with following parameters :
			  \begin{itemize}
		      	\item \textbf{Image} : frame
		      	\item \textbf{ScaleFactor} : 1.1
		      	\item \textbf{MinNeighbors} :  2
		      \end{itemize}	
		\item draw bounding boxes on frame
		\item display frame		
  	\end{enumerate}
  \end{itemize}
\end{algorithm}


As expected, the first implementation of face/eyes detection with haar cascades in real-time using a laptop camera was very fast and computation friendly. Despite of the speed, both detectors showed few FPs (False positives) see Figure\ref{fig:Fedwhc}.

\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{f0}
\caption{face/eyes detection with Haar cascades}
\label{fig:Fedwhc}
\end{figure}


So, By applying haar-like detectors for face and eyes detection, we gained time and speed which allows us to use this method in real-time driver monitoring. However, we also need further improvements, as we still may encounter issues of either missing detections (FNs) or false detections (FPs). In the case of this study, FNs does not a serious problem because, in the worst case they only invoke alerts, Unlike FPs. False-positive means the driver is maybe distracted but the system can detect the face/eyes of the driver and this is too dangerous because no alert will be invoked in this case. So, in the optimization section, we will focus more on decreasing the (FPs), and improving algorithm robustness under variable lighting conditions.




\subsection{Problems and limitations}

Haar cascades are notoriously prone to false positives, the Haar-like classifier can easily report a face in an image when no face is present under normal lighting conditions, and the eye classifier is worse in false detection because there are many parts of the face have the same color and shape property of the eyes, for example, the \emph{Oral commissure}\footnote{The commissure is the corner of the mouth, where the vermillion border of the superior labium (upper lip) meets that of the inferior labium (lower lip).}. The situation becomes even more complicated when a part of the driver's face is brighter than the other part (due to light falling in through a side window), making eye status detection extremely difficult. In addition to that we noticed that sometimes the performance drops dramatically after a while due to 
the big number of computations per frame.

Another important thing to note here is, along with the testing of this method we found a problem which is sometimes a face or an eye can be detected 2 times in the same instance. this situation appears generally when trying to approach the camera, suddenly a bounding box appears in the screen including both the object (face / eye ), and the other bounding box.

In the next section, we propose a few methods to tackle the above issues which mainly summarized in :

\begin{itemize}

\item False positives for both face and eyes

\item double detection, this problem is when an object has been detected two times in the same instance

\item detection fails under bad lighting conditions

\end{itemize}


\subsection{Optimization}

Haar-based detectors use haar features to detect objects. So it is very common to find an object in the image that has the same properties as the face, this problem has also been encountered in eye detection. In the first part of this section, we focus to improve the accuracy of the algorithm by reducing Fps detections. In the second part, we define a method to avoid the double detection problem based on spatial coordinates. Then we propose a night vision mode to improve the algorithm robustness under bad lighting conditions. Finally we work on improving the speed of the algorithm.\\


Starting with the double detection problem illustrated in Figure \ref{fig:Ddetection}. to solve this problem we propose a simple function, which uses spatial parameters ( x , y , width , height ) returned from the detector to check if there is a bounding box that includes another bounding box which means the two boxes are for the same object if this is the case the function ignores the big box. We use this function for both eye and face detection. 
To reduce False detections for eyes, we added another function named check\_ eyes, which orders the detected objects List with respect of "y" axis values and return the first two objects. With this function, we reduced false positives for eye detection.

\begin{figure}[!h]
\centering
\includegraphics[width=.4\textwidth]{include}
\caption{Double detection problem}
\label{fig:Ddetection}
\end{figure}

The main factors that control the object detection process are the detector's arguments (ScaleFactor, MinNeighbors, MinSize, MaxSize). Starting with ScaleFactor, since the classifier was trained with fixed object size, it can detect the object with the same size that was used during the training phase \parencite{ScaleFactorImagePyramide}. Usually in the application phase, the size of the object may be smaller or larger than what was used in training, this maybe related to the distance between the object and the camera. To ensure the detection of an object in different scales\footnote{Scale is the relative size of different objects in relation to each other or a common standard \parencite{Scale} } there are two terminologies: Image Pyramid, and Scale Factor.

Image pyramid and ScaleFactor specifies how much we reduce the image size each time we scale (i.e., ScaleFactor = 1.5 means reducing the input image size by 5\% ). Or, how much we increase the sliding window size (i.e., ScaleFactor = 1.2 means increasing window size by 20\% at each iteration), see Figure\ref{fig:IPASF}. 

\begin{figure}[!h]
\centering
\includegraphics[width=.7\textwidth]{pyramid}
\caption{Image Pyramid and Scale Factor}
\label{fig:IPASF}
\end{figure}

Generally, the smaller the ScaleFactor (SF), the more detailed the research, and the higher the calculation costs.

As object detection works within the image pyramid combination (multi-scale) , the detector can find multiple True response for a single region of an object. These responses are object candidates in a given region, here the detector use the Minimum Neighbors (MNN) parameter to confirm the detection by summing the number of rectangles candidates in that region this value must be higher than the MNN to decide whether there is an object or not. So, MNN is a parameter that can enhance algorithm accuracy in case of using high values, thus reduce detection rate. Also, decreasing MNN value increase of detection rate which means increasing the false-positives (FPs) rate as well See Figure \ref{fig:MNNINITVALUES}.


\begin{figure}[!h]
\centering
  \subcaptionbox{Face MNN = 0 , Eye MNN = 0}{\includegraphics[width = .3\textwidth]{MNNF0E0}}\quad
  \subcaptionbox{Face MNN = 0 , Eye MNN = 1}{\includegraphics[width = .3\textwidth]{MNNF1E0}}\quad
  \subcaptionbox{Face MNN = 1 , Eye MNN = 1}{\includegraphics[width = .3\textwidth]{MNNF1E1}}
  \caption{detections using various MNN values. Dir eyes whdha w face whdha ! }
\label{fig:MNNINITVALUES}
\end{figure}


There are also two additional parameters which are Minsize and Maxsize, their utility is to control the detection by the size of the detected object ( width , height ) if the size falls outside the range [minsize ... maxsize] the detection is ignored, only objects which have a size in that range is accepted. These arguments are very important for decreasing FPs detection rate. At this point, we supposed that the driver will be sitting about 1m far from the camera, and we recorded the width and height of the detected face and eyes, in general, the distance between the driver's face and the camera is constant this is what insure that we don't need a dynamic values for MinSize and MaxSize only in specific cases like changing the vehicle in this case we can simply adjust the camera's position.

\begin{figure}[!h]
\centering
  \subcaptionbox{distance = 20cm}{\includegraphics[width = .3\textwidth]{20cm}}\quad
  \subcaptionbox{distance = 60cm}{\includegraphics[width = .3\textwidth]{60cm}}\quad
  \subcaptionbox{distance = 100cm}{\includegraphics[width = .3\textwidth]{100cm}}
  \caption{Face/Eyes sizes for different distances of the driver and the camera}
\label{fig:Sizes}
\end{figure}



After finding the good values for ( MaxSize, MinSize ) relative to the distance between the driver and the camera, these values help us to reduce FPs rate by ignoring False detections based on the face/eye size.

Now we need to find optimum parameters for ScaleFactor and Minimum Neighbors, in this step we recorded SF and MNN values under different conditions and we realized that these parameters have a relation with luminosity level and the distance so they can not be constants. Instead, we need to record the best values for different situations especially lighting conditions because in general, the distance does not change, then makes these parameters dynamic with the help of criteria that will estimate the luminosity level and decide which couple of values for SF and MNN are optimum based on the estimated luminosity level.

For the luminosity level estimation, there are many different methods like deep learning model for \emph{Day-Night Classification} \parencite{DayNight}. even though the deep learning approach is the most accurate for this specific task, it adds more computation costs which will slow down the algorithm. So, we used the image basic statistics to solve this issue. Figure \ref{fig:Fedwhcudlc} shows face/eyes detection with haar cascades under non-ideal lighting conditions.


\begin{figure}[!h]
\centering
  \subcaptionbox{}{\includegraphics[width = .2\textwidth]{f1}}\quad
  \subcaptionbox{}{\includegraphics[width = .2\textwidth]{f4}}\quad
  \subcaptionbox{}{\includegraphics[width = .2\textwidth]{f3}}\quad
  \subcaptionbox{}{\includegraphics[width = .2\textwidth]{f2}}
  \caption{face/eyes detection with Haar cascades under diferent lighting conditions}
\label{fig:Fedwhcudlc}
\end{figure}


In images (a) and (b), the detection was performed for the face and both eyes, but in the image (c), only the face was detected successfully, both detectors failed in the image (d). For a first test we assumed that the image (b) is the minimum luminosity value for face/eye detection, so we calculated basic statistics (Mean , Variance , Energy) for each image in Figure \ref{fig:Fedwhcudlc} in grayscale color system.


\begin{table}[!h]
\centering

\begin{tabular}{ | c | c | c | c | c | } 
\hline
 & image (a) & image (b) & image (c) & image (d) \\
\hline
Mean     & 68.47 & 13.82 & 6.71 & 5.66 \\
\hline
Variance & 2615.06 & 428.27 & 86.71 & 54.06 \\
\hline
Energy   & 111.55 & 42.29 & 31.41 & 29.59 \\
\hline
\end{tabular}
\caption{Table shows basic statistics for grayscale images of Figure \ref{fig:Fedwhcudlc} under different lighting conditions}
\label{table:BasicStatisticsForGrayScaleImages}
\end{table}

we need a criterion to classify lighting conditions into (good/bad). According to the table above, the Mean and Energy show a difference between the images, but we cannot use them because the difference between the values of the image (b) (minimum) and the image (c) of the Mean or Energy is too small and can be easily affected by light, however, the Variance shows a large difference between the image (b) and the image (c). So we used the Variance as a criterion for the classification of lighting conditions.\\


To find the optimum values in different conditions which were done in terms of TP and FP rates with the help of python programming language which allows us to see algorithm results with the possibility of changing parameters values in real-time.

\begin{table}[!h]
\centering

\begin{tabular}{ |c|c|c|c|c|c|c|  }
 \hline
 & \multicolumn{2}{|c|}{Good LC} & \multicolumn{2}{|c|}{Bad LC}& \multicolumn{2}{|c|}{All LC}\\
 \hline
 & SF & MNN & SF & MNN &MinSize&MaxSize\\
 \hline
 Face & 1.3 & 1 & 1.1 & 1 & (116,116) &(331,331) \\
 \hline
Eye & 2.2 & 3 & None & None & (44,44) &(44,44) \\
 \hline
\end{tabular}

\caption{Table shows optimum values for detector parameters under different lighting conditions}
\label{table:OptimumValuesParameters}
\end{table}

Due to the failure of eyes detection under bad lighting conditions, we propose a night vision system based on image processing technique called "Power Law Transformation" (Gamma Correction) with the help of the Variance. Gamma correction is a nonlinear adaptation applied to each and every pixel value. Generally, linear methods like addition, subtraction, and multiplication are applied to all the pixels. Gamma correction is responsible for performing nonlinear methods on the pixels of the input image and thereby remodeling the saturation of the image \parencite{THARSANEE2021117}. To enhance the quality of the image we must loop over all pixels of the image by applying the power transformation equation:

 $$I'(i,j) = C \quad * \quad I(i,j)^{\quad \lambda}$$

Where C and $\lambda$ are positive constants and I is the intensity value of the
pixel in the input image at (i, j), and I' is the output intensity value. In most cases C = 1. if $\lambda$ = 1 then the mapping is linear and the output (I') image is the same as the input image. When $\lambda$ $<$ 1, the output image appears lighter than the input image. When $\lambda$ is bigger than 1, The effect is opposite, where the output image appears darker than the input. See  Figure\ref{fig:lambdaeffect} 

\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{lambda}
\caption{the effect of different values of $\lambda$ for C = 1.}
\label{fig:lambdaeffect}
\end{figure}

Another approach \parencite{GPC} of the gamma correction is used in the application phase. First, our image must be scaled from the range [0 , 255] to [0.0 , 1.0]. Then we calculate the output image I' by applying the following equation:

 $$I'(i,j) =\quad I(i,j)^{\frac{1}{\gamma}}$$
 

 In this case, the output image will be lighter when $\gamma$ is $>$ 1 . And $\gamma$ values $<$ 1 will make the  output image appears darker than the input. See Figure \ref{fig:gammac}

 
\begin{figure}[!h]
\centering
  \includegraphics[width=.8\textwidth]{gammac}
  \caption{Power Law Transformation effect}
\label{fig:gammac}
\end{figure}


\newpage
  So based on the variance of a given frame in gray scale system we can check if it presents a good lighting conditions for face/eyes detection or we need to correct the luminance of the frame with the Power Law Transformation.
 
\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{nightvision}
\caption{Flowchart illustrates the proposed night vision system based on variance and gamma correction}
\label{fig:nightvisionsystem}
\end{figure}


\begin{algorithm}[!h]
  \caption{Algorithm used in the proposed night vision system based on variance and gamma correction}
  \begin{itemize}
  	\item Foreach frame from video input :	
  	\begin{enumerate}	
		\item Change color space from RGB to Gray
		\item Variance calculation : 
$$ Variance  =  \frac{1}{XY}  \sum_{i=1}^{X}  \sum_{j}^{Y}  | I(i,j) - Mean(I)|^2  $$
		
		\item If ( Variance $<$ th ) :
			  \begin{itemize}
			  
				\item Change value of each pixel in the frame I by:
				 $$I(i,j) =\quad I'(i,j) =\quad I(i,j)^{\frac{1}{\gamma}}$$
				
			  \end{itemize}
			  		
  	\end{enumerate}
  \end{itemize}
\end{algorithm}

Using Variance threshold = 350 and a $\lambda$ value = 0.22, and the optimum detector parameters under bad lighting conditions we got the following result:

\begin{figure}[!h]
\centering
  \subcaptionbox{}{\includegraphics[width = .4\textwidth]{dnvbefor}}\quad
  \subcaptionbox{}{\includegraphics[width = .4\textwidth]{dnvafter}}\quad
  \caption{Face / eye detection under different lighting conditions}
\label{fig:nvbeforafter}
\end{figure}


To improve the algorithm in terms of speed and calculations complexity, we decide to run the eye detector only if we find a face. And make it search in the ROI which is in this case the face region.

Finally, to invoke alarms we devided the driver state into four possible situations and enimurate these states to make the algorithm returns a number and based on that number we can invoke alarms. See Figure \ref{fig:lastflowchart}

\begin{figure}[h]
\centering
\includegraphics[width=.7\textwidth]{lastflowchart}
\caption{Flowchart illustrates the proposed driver monitoring algorithm based on Haar cascades}
\label{fig:lastflowchart}
\end{figure}
 

- 0 : the driver is awake and focusing on the road 

- 1 : the driver is showing a sign of drowsiness 

- 2 : the driver is drowsy 

- 3 : the driver is distracted 




\newpage
\section{method 2 : Facial key landmarks}
\emph{Face landmark detection is a computer vision task where we want to detect and track key points from a human face. This task applies to many problems. For example, we can use the key points for detecting a human’s head pose position and rotation. With that, we can track whether a driver is paying attention or not. In this section, we use facial key landmarks to detect eye blinks in real-time and then decide whether the driver is awake or drowsy.}

Many approaches for detecting eye blinks in a video series have been proposed. A motion estimate in the ocular area is used in several approaches. Typically, the face and eyes are detected by a Viola-Jones type detector. Next, motion in the eye area is estimated from optical flow, by sparse tracking \parencite{Soukupov2016RealTimeEB}. A major drawback of the previous approaches is that they usually very sensetive for image resolution, illumination, motion dynamics, etc.

Robust real-time facial landmark detectors that capture most of the characteristic points on a human face image, including eye corners and eyelids, are available, we can use the prebuilt library like dlib, OpenCV, and mediapipe. These modern landmark detectors are trained on “in-the-wild datasets” like "Helen", "300W", and "LFPW" datasets, and they are thus robust to varied lighting conditions, facial emotions, and non-frontal head rotations. The average error of a state-of-the-art detector's landmark localization is generally less than 5\% of the inter-ocular distance. Recent methods run even significantly super real-time.

In this method, we use the dlib model for facial key landmarks detection for drowsiness detection. This model is based on ensemble regression trees because the model will predict continuous numbers. On the other hand, we use a model based on HOG and SVM for face detection.

In the context of drowsiness detection, our goal is to detect eyes then evaluate the drowsiness level or in other words, check eye's state (opened/closed). The facial landmark detector inside the dlib library is used to detect the location of 68 points of (x,y) coordinates, these points can be visualized in Figure \ref{fig:dlibskeys}. 


%% Figure of dlib's model (key points and indexes

\begin{figure}[!h]
\centering
  {\includegraphics[width = .5\textwidth]{dlibs68face}}
  %\subcaptionbox{}{\includegraphics[width = .4\textwidth]{dnvafter}}\quad
  \caption{the 68 facial landmark coordinates from the iBUG 300-W dataset}
\label{fig:dlibskeys}
\end{figure}


\subsection{Implementation}

The first implementation of this method was done in a similar way as the previous. First, we receive frames one by one from the camera then we pass the frame to the face detection model which will calculate HOG descriptor with the help of SVM the model detects the face region if there is a face and returns bounding box parameters. After that we extract the ROI (face region) and give it to the facial key landmark model which will predict the 68 points that map to facial structures on the face.

The face detection was very accurate and stable compared to haar cascades, also the facial key landmark detection was very fast and accurate even though a part of the face is hidden See Figure .. X


Each eye is represented by 6 (x, y) coordinates, going clockwise around the rest of the area, commencing at the left corner of the eye.


% eye key points

For the detection of sleepiness, we used the Euclidean distance to decide whether the eye is open or closed depending on the distance between the points above and botom. Unfortunately, this technique was less accurate than the work of Soukupová and Čech \parencite[soukupova2016eye] in their equation that reflects this relationship called the eye aspect ratio (EAR), this equation calculates the distance between the vertical marks of the eye while the denominator calculates the distance between the horizontal marks of the eye, weights the denominator appropriately since there is only one set of horizontal points, but two sets of vertical points see Figure \ref{fig:EAR}.

% EAR 

\begin{figure}[!h]
\centering
  {\includegraphics[width = .8\textwidth]{EARthershold}}

  \caption{Top-left: A visualization of eye landmarks when then the eye is open. Top-right: Eye landmarks when the eye is closed. Bottom: Plotting the eye aspect ratio over time. The dip in the eye aspect ratio indicates a blink \parencite{soukupova2016eye}.}
\label{fig:EAR}
\end{figure}

The eye aspect ratio equation : $$EAR \quad = \quad \frac{||p_2 \quad - \quad p_6|| \quad + \quad ||p_3 \quad - \quad p_5||}{2 \quad ||p_1 \quad - \quad p_4||}$$


\subsection{Problems and limitations}

Like most computer vision tasks, this method is sensitive to lighting conditions and the distance between the camera and the driver, these two factors seriously affect the decision if the eyes are open or closed, The other problem is that this method drops considerably if the driver wears sunglasses because the eye marks take approximate coordinates in relative to the mouth and other areas of the face


% here put figures of problems and limitations 


\subsection{Optimization}

To improve performance, we have included some functions to avoid double detection as we did in the previous method although this problem is not common in this method. For the lighting conditions, we used the gamma correction technique, and it works perfectly with the detection of facial key landmark. also, we kept only the views of the 68 key points which helped the algorithm by reducing calculations cost.

At this level, we added limits to the driver to detect bad behaviors such as risky positions, these limits are implemented in two levels orange  and green areas. By default, the limits are in the middle of the frame. We added a function that changes the positions of the limits in order to make theme work in any type of cars see Figure X


% lahna 7at tsawar ta3 les limits w m3ahom limit ray7in l lagaush



\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{facialkeyflowchart}
\caption{Flowchart illustrates the proposed driver monitoring algorithm based on Facial Landmarks}
\label{fig:facialflowchart}
\end{figure}


\section{Conclusion}

We have discussed in the chapter methods to assess the driver's state of drowsiness and inattention based on face and eyes-status analysis followed by optimization techniques to improve the performance of each method. We have noted that 'speed' and 'lighting condition' are the main challenges in this field.

First, we used haar cascades classifiers and we restricted ourselves to the detection results only to evaluate the driver state. Even though this method showed an important result in terms of speed and implementation cost but it drops dramatically under bad lighting conditions also camera vibrations, especially for eye detection which makes this method a bad choice in real-time driver monitoring. Then we used the facial Landmarks detector which is more accurate and allow us to do further calculations to evaluate the driver's state, this method showed impressive results in terms of accuracy and robustness.

Finally, we have combined the driver’s state with an alarm module that is responsible for making alerts to wake the driver in dangerous situations.















