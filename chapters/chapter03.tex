\chapter{Driver Monitoring}

To Update !
\emph{In this chapter, we propose methods to assess the driver's state of drowsiness and inattention based on face and eyes-status analysis. The chapter  begins with a brief discussion of signs of drowsiness and available methods for detecting them, and it continues with the first proposed method which is based on traditional computer vision techniques followed by the seconde method which is based on deep learning, then we discuss the strengths, weakness, and limitations for each method. The chapter continues with our optimization techniques to improve the performance of these methods in terms of speed, detection rate, and detection accuracy under non-ideal lighting conditions and for noisy images. } %% to edit


\section{Introduction}
A driver-monitoring system is an advanced safety feature that track driver drowsiness or distraction, and to issue a warning or alert to get the driver’s attention back to the task of driving.

Driver-monitoring systems typically use sensors to collect data about the driver and pass these data to a software. The software can then determine whether the driver is blinking more than usual, whether the eyes are narrowing or closing, and whether the head is tilting at an odd angle. It can also determine whether the driver is looking at the road ahead, and whether the driver is actually paying attention or just absent-mindedly staring.

\section{Driver Monitoring Technologies}
According to \parencite{DrowsinessDetectionSystem} there are many approaches and measurement technologies to predict driver's behaviors. The most commonly used measurement can be categorized upon the monitoring instrument into : 
\begin{itemize}
	\item Video-based sensors
	\item Physiological signals sensors 
\end{itemize}

\begin{figure}[!h]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{cmsensors}  
  \caption{Video-based sensors (used in this thesis)}
  \label{fig:CamSensors}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{phsensors}  
  \caption{physiological signals sensors}
  \label{fig:PhSensors}
\end{subfigure}
\caption{Illustration of difrent Driver monitoring approches}
\label{fig:DifrentMonitoringApproches}
\end{figure}


\subsection{Physiological Signals Sensors}
Physiological signals of the driver are the most accurate solutions, they can be used to measure his vigilance level since these signals originated from human organs such as brain, eyes, muscles, and heart that can indicate the fatigue and alertness level in real-time as depicted in Figure \ref{fig:PhSensors}. Physiological measures can be recorded from different organs that show visible correlation with the wakefulness/drowsiness state of a person. This includes \parencite{DrowsinessDetectionSystem}:

	\begin{itemize}
		\item \textbf{Brain activity},which can be captured by electroencephalography (EEG) or Near Infrared Spectroscopy
(NIRS).
		\item \textbf{Cardiac activity}, monitored through electrocardiography (ECG) and Blood Pressure signals.
		\item \textbf{Ocular activity}, measured by electrooculography (EOG)
	\end{itemize}

\subsection{Video-based sensors}
To determine alertness/drowsiness level of driver as illustrated in Figure \ref{fig:CamSensors}. The behaviour of the driver is mainly monitored through a camera and thus this approach is known as video-based measure. Visible symptoms of fatigue and sleepiness can be observed when driver becomes drowsy through measuring its abnormal behaviours. Research on fatigue and drowsiness detection using driver behavioural monitoring focused on three main measure: \emph{Eyes state}, \emph{Face expression}, and \emph{Head position}.

\subsection{Evaluation}
\emph{The following evaluation and ranking are based on our online search of driver monitoring technologies and we belive that it is not the only evaluation method.}

According to \parencite{DrowsinessDetectionSystem}, physiological sensors make it possible to alert driver at earlier stages of drowsiness and thereby prevent many drastic accidents \parencite{EvaluationOfTechnologies}.  Physiological measures have been shown to be reliable and accurate since they are less impacted by environmental and road conditions and thus may have fewer false positives \parencite{Zilberg2007MethodologyAI}.

On the other hand, video sensors technology is user friendly and can be mounted comfortably in various areas inside a vehicle also,it has the lowest coast. The common limitation is lighting conditions.

\begin{table}[h!]
\centering

\begin{tabular}{ |c c c| } 
\hline
Technology & Video-Based Sensors & Physiological Signals Sensors \\
\hline
Coast & ++ & +\\
Ease of Use & +++ & + \\
Intrusiveness & + & +++ \\
Accuracy & ++ & +++\\
\hline
\end{tabular}
\caption{shows the evaluation results of the two above described Driver Monitoring Technologies. The (+) symbol represents the rating level}
\label{table:DriverMonitoringTEchnologiesEvaluationResult}
\end{table}



\section{Driver Drowsiness}
Feeling sleepy or tired during the day is commonly called drowsiness. Drowsiness can lead to additional symptoms, such as forgetting or falling asleep at inappropriate times, especially in the case of driving because it leads to a car accident.
Drowsiness in general is accompanied by warning signs that differ from one person to another, such as yawning\footnote{yawning is a response to fatigue,  it is characterized by opening up of mouth which is accompanied by a long inspiration, with a brief interruption of ventilation and followed by a short expiration.} or blinking frequently, nodding\footnote{nodding also is a response to fatigue, it is characterized by lower or raising the head slowly and briefly}, drifting off the track, and the most critical sign of drowsiness is closed eyes.\parencite{DrowsinessSigns}

\section{Driver Inattention}
Driver inattention or distraction occurs when a driver engages in a secondary activity that interferes with the primary task of operating a vehicle. Drivers can be distracted in many ways by things inside or outside of the vehicle. There are three categories in which driver distraction \parencite{DriverInnatention}:


\begin{enumerate}
\item \textbf{Visual} : taking your eyes off the road, such as :
						
		\begin{itemize}						
			\item Texting
			\item Other distractions outside the vehicle
		\end{itemize}
																			
					
\item \textbf{Physical} : taking your hands off the steering wheel, like :
		
		\begin{itemize}						
			\item Adjusting vehicle controls, such as the air conditioner
			\item Eating or drinking
		\end{itemize}

\item \textbf{Cognitive} : taking your attention away from the driving task. Cognitive distraction usually accompanies physical and visual distractions.
\end{enumerate}

\begin{figure}[!h]
\centering
\includegraphics[width=.99\textwidth]{distracteddrivers}
\caption{Distracted Driver Sceenes}
\label{fig:DDS}
\end{figure}

the common sign of driver inattention is the head position, when the driver looks in a direction other than the road it means he is not focusing on the road and this is too dangerous see Figure\ref{fig:DDS}.


\newpage
\section{Proposed work}
\emph{In this section, we propose methods for detecting driver’s drowsiness and distraction using computer vision techniques discussed in chapter 2 with other algorithms that aim for improve performance and accuracy (ACC), then !!!!!!!! gol beli rah ndiro comparaison w rah ndiro analyse l kol methode w nokhorjo b natija ... (!!!! ahki ela system d'alarme)  }

Before anything, the main goal of this work is to invoke audio-alert when a bad thing happend .. 
!!!!

For the application phase, the driver’s surveillance camera is mounted in front of the driver’s face it can be placed in the areas of the steering wheel or it can be hung on the rearview mirror.



\section{method 1 : Haar Cascades}
\emph{Viola-Jones facial detection technique, commonly known as Haar Cascades. This work was done well before the beginning of the era of deep learning. But it’s a great job in comparison with powerful models that can be built with modern deep learning techniques, especially in terms of speed. The algorithm is still used almost everywhere.}\\

Haar Cascades in general is an object detection algorithm uses haar features. Haar Features were not only used to detect faces, but also for eyes, lips, license number plates, etc. \parencite{HaarFeaturesUses}.\\

For a good detection rate, we need a strong classifier that is trained in using a large set of \emph{positive}\footnote{Positive data points are examples of regions containing a face} and \emph{negative}\footnote{Negative data points are examples of regions that do not contain a face} samples of a face, however, \emph{OpenCV}\footnote{Open Source Computer Vision, is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez.} can perform face detection out-of-the-box using a pre-trained Haar cascade that is mean OpenCV's Haar cascade has already picked the best haar-like features for face detection, eyes detection, etc.\\

This ensures that we do not need to provide our own positive and negative samples, train our own classifier, or worry about getting the parameters tuned exactly right. Instead, we need to focus on improving speed, accuracy and finding solutions for challenging conditions \parencite{PyImageSearchHaarCascades} \parencite{OpenCvHaarCascades} \parencite{HaarFeaturesUses}.

In the application of this method, we use Haar-like detectors provided by \emph{OpenCv}, a haar-like detector takes as argumments \parencite{OpenCvHaarCascades}:

\begin{itemize}

	\item \textbf{Image}: matrix of the type \emph{CV\_8U} \footnote{CV\_8U is unsigned 8bit pixel, a pixel can have values 0-255, this is the normal range for most image and video formats.} containing an gray-scale image where objects are detected.

	\item \textbf{ScaleFactor}: parameter specifying how much the image size is reduced at each image scale.

	\item \textbf{MinNeighbors}: parameter specifying how many neighbors each candidate rectangle should have to retain it.

	\item \textbf{MinSize}: minimum possible object size. Objects smaller than this are ignored.
	
	\item \textbf{MaxSize}: maximum possible object size. Objects larger than this are ignored.

\end{itemize}

By default a haar-like detector returns 4 values \parencite{OpenCvHaarCascades} x-coordinate, y-coordinate, width(w), height(h) of the detected target object, these 4 values represent 2 spatial points $(x , y)$ and $(\quad x + w ,\quad y + h) $ of the rectangle that contains the object see Figure \ref{fig:bbprincipe}. 

\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{bbprincipe}
\caption{Illustration of Bounding Box in object detection using haar-like detector}
\label{fig:bbprincipe}
\end{figure}
 
Taking the Figure \ref{fig:bbprincipe} as an example. After a successful object detection, a haar-like detector returns :  $(x,\quad y, \quad w, \quad h) = (20, \quad 40, \quad 40, \quad 40)$ respectively.So, $p1 = (20, \quad 40)$ and $p2 = (20 + 40 , 40 + 40 ) = (60 ,\quad 80)$. However, these bounding boxes are useless in the case of driver monitoring, the most important thing is to alert the driver in real time when he is asleep or distracted.

In the application stage of this method to serve as a driver monitoring technique. we used two Haar-like detectors, the first one is a face detector to check whether a given image contains a face or not, In other words, the face detector can check whether the driver is focusing on the road or distracted. Assuming the camera is in front of the driver’s face, if the face detector finds a face on a given image, This means that the driver looks forward and if the detector cannot find a face that means that the driver is distracted by looking in a direction other than the road, which is a dangerous situation, especially when driving fast. 

The second Haar-like detector is an eye detector, which can find an "open" eye in a given image. Using this condition, the eye detector can check if the driver’s eyes are open or closed, simply by performing an eye detection if the detector finds one or both eyes that means the driver is focusing on the road if not, This means that one or both eyes of the driver are closed and this is the critical sign of driver's drowsiness which requires an alert to wake up the driver.
 
As this method works with images, and the camera feeds the system with a video stream which is basically a sequence of images we processed the video frame by frame, and for each frame, we performed haar cascades see Figure\ref{fig:Initfchc}. 

\begin{figure}[!h]
\centering
\includegraphics[width=.7\textwidth]{Init haarcascades}
\caption{Initial flowchart for driver monitoring with haarcascades}
\label{fig:Initfchc}
\end{figure}

\newpage

 The first implementation of this method was done with the following algorithm under difrent lighting conditions :  
 
 
\begin{algorithm}[!h]
  \caption{first algorithm for face/eyes detection using Haar cascades}
  \begin{itemize}
  	\item Load haar-like detectors provided by OpenCv
  	\item Foreach frame from video input :	
  	\begin{enumerate}	
		\item Change color space from RGB to Gray	
		\item Face detection with following parameters : 
		      \begin{itemize}
		      	\item \textbf{Image} : frame
		      	\item \textbf{ScaleFactor} : 1.1
		      	\item \textbf{MinNeighbors} :  2
		      \end{itemize}	     
		\item Eye detection with following parameters :
			  \begin{itemize}
		      	\item \textbf{Image} : frame
		      	\item \textbf{ScaleFactor} : 1.1
		      	\item \textbf{MinNeighbors} :  2
		      \end{itemize}	
		\item draw bounding boxes on frame
		\item display frame		
  	\end{enumerate}
  \end{itemize}
\end{algorithm}


As expected, the first implementation of face/eyes detection with haar cascades in real-time using a laptop camera was very fast and computation friendly. Despite of the speed, both detectors showed few FPs (Flse positives) see Figure\ref{fig:Fedwhc}.

\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{f0}
\caption{face/eyes detection with Haar cascades}
\label{fig:Fedwhc}
\end{figure}


So, By applying haar-like detectors for face and eyes detection, we gained time and speed wich allows us to use this method in real-time driver monitoring. However, we also need further improvements, as we still may encounter issues of either missing detections (FNs) or false detections (FPs). In the case of this study FNs does not a serious problem because, in the worst case they only invoke alerts, Unlike FPs. False-positive means the driver is maybe distracted but the system can detect the face/eyes of the driver and this is too dangerous because no alert will be invoked in this case. So, in the optimization section we will focus more on decreasing the (FPs), and improving algorithm robustness under variable lighting conditions.




\subsection{Problems and limitations}

Haar cascades are notoriously prone to false-positives, the Haar-like classifier can easily report a face in an image when no face is present under normal lighting conditions, eye classifier is worse in false detection because there are many parts of face have the same color and shape property of the eyes for example the \emph{Oral commissure}\footnote{The commissure is the corner of the mouth, where the vermillion border of the superior labium (upper lip) meets that of the inferior labium (lower lip).}. The situation becomes even more complicated when a part of the driver's face is brighter than the other part (due to light falling in through a side-window), making eye status detection extremely difficult. In addition to that we noticed that sometimes the performance drops dramaticaly after a while due to 
the big number of computations per frame.

Another important thing to note here , along the testing of this method we found a problem which is sometimes a face or an eye can be detected 2 times in the same instance. this situation apears generraly when trying to approach the camera, sudenly a bounding box apears in the screen including both the object (face / eye ), and the other bounding box.


In the next section we propose few methods to tackle the above issues which mainly summarized in :

\begin{itemize}

\item False positives for both face and eyes

\item double detection, this problem is when an object has been detected two times in the same instance

\item detection fails under bad lighting conditions

\end{itemize}


\subsection{Optimization}

\emph{Haar-based detectors use haar features to detect objects. So it is very common to find an object in the image that has the same properties as the face, this problem has also encountered in eye detection. In the first part of this section, we focus to improve the accuracy of the algorithm by reducing Fps detections. In the second part, we define a method to avoid the double detection problem based on spatial cordinates. Then we propose a night vision mode to improove the algorithm robustness under bad lighting conditions.Finally we work on improoving speed of the algorithm.}\\


Starting with the double detection problem illustrated in Figure \ref{fig:Ddetection}. to solve this problem we propose a simple function, wich uses spatial parameters ( x , y , width , height ) returned from the detector to check if there is a bounding box includes another bounding box wich means the two boxes are for the same object if this is the case the function ignores the big box. We use this function for both eye and face detection. 
To reduce False detections for eyes, we added another function named check\_ eyes, wich order the detected objects List with respect of "y" axis values and return the first two objects. With this function we reduced false postives for eye detection.

\begin{figure}[!h]
\centering
\includegraphics[width=.4\textwidth]{include}
\caption{Double detection problem}
\label{fig:Ddetection}
\end{figure}

The main factors that control the object detection process are the detector's arguments (ScaleFactor, MinNeighbors, MinSize, MaxSize). Starting with ScaleFactor, since the classifier was trained with a fixed object size, it can detect the object with the same size that was used during the training phase \parencite{ScaleFactorImagePyramide}.Usually in the application phase, the size of the object may be smaller or larger than what was used in training, this may be related to the distance between the object and the camera. To ensure the detection of object in different scales\footnote{Scale is the relative size of different objects in relation to each other or a common standard \parencite{Scale} } there are two terminologies: Image Pyramid, and Scale Factor.

Image pyramid and ScaleFactor specifies how much we reduce the image size each time we scale (i.e., ScaleFactor = 1.5 means reduce the input image size by 5\% ). Or, how much we increase the sliding window size (i.e., ScaleFactor = 1.2 means increase window size by 20\% at each iteration), see Figure\ref{fig:IPASF}. 

\begin{figure}[!h]
\centering
\includegraphics[width=.7\textwidth]{pyramid}
\caption{Image Pyramid and Scale Factor}
\label{fig:IPASF}
\end{figure}

Generally, the smaller the ScaleFactor (SF), the more detailed the research, and the higher the calculation costs.

As object detection works within the image pyramid combination (multi-scale) , the detector can find multiple True response for a single region of an object. These responses are object candidates in a given region, here the detector use the Minimum Neighbors (MNN) parameter to confirm the detection by summing the number of rectangles candidates in that region this value must be higher than the MNN to decide whether there is an object or not. So, MNN is a parameter that can enhance algorithm accuracy in case of using high values, thus reduce detection rate. Also, decreasing MNN value increase of detection rate wich means increasing the false-positives (FPs) rate as well See Figure \ref{fig:MNNINITVALUES}.


\begin{figure}[!h]
\centering
  \subcaptionbox{Face MNN = 0 , Eye MNN = 0}{\includegraphics[width = .3\textwidth]{MNNF0E0}}\quad
  \subcaptionbox{Face MNN = 0 , Eye MNN = 1}{\includegraphics[width = .3\textwidth]{MNNF1E0}}\quad
  \subcaptionbox{Face MNN = 1 , Eye MNN = 1}{\includegraphics[width = .3\textwidth]{MNNF1E1}}
  \caption{detections using various MNN values. Dir eyes whdha w face whdha ! }
\label{fig:MNNINITVALUES}
\end{figure}


There are also two additional parameters which are Minsize and Maxsize, thier utility is to control the detection by the size of the detected object ( width , height ) if the size fall outside the range [minsize ... maxsize] the detection are ignored, only objects which have size in that range are accepted. These arguments are very important for decreasing FPs detection rate. In this point we supposed that the driver will be sitting about 1m far from the camera, and we recorded the width and height of the detected face and eyes, in general the distance between the driver's face and the camera is constant this what insure that we dont need a dynamic values for MinSize and MaxSize only in a specifique cases like changing the vheicle in this case we can simply adjust camera's position.

\begin{figure}[!h]
\centering
  \subcaptionbox{distance = 20cm}{\includegraphics[width = .3\textwidth]{20cm}}\quad
  \subcaptionbox{distance = 60cm}{\includegraphics[width = .3\textwidth]{60cm}}\quad
  \subcaptionbox{distance = 100cm}{\includegraphics[width = .3\textwidth]{100cm}}
  \caption{Face/Eyes sizes for diffrent distances of the driver and the camera}
\label{fig:Sizes}
\end{figure}



After finding the good values for ( MaxSize, MinSize ) relative to the distance between the driver and the camera, these values help us to reduce FPs rate by ignoring False detections based on the face/eye size.

Now we need to find optimum parameters for ScaleFactor and Minimum Neighbors, in this step we recorded SF and MNN values under diffrent conditions and we realized that these parameters have a relation with luminosity level and the distance so they can not be constants. Instead we need to record the best values for diffrent situations esspecially lighting conditions because in general the distance do not change, then make these parameters dynamique with the help of a criterian that will estimate the luminosity level and decide which couple of values for SF and MNN are optimum based on the estimated luminosity level.

For the luminosity level estimation, there are many diffrent methods like deep learning model for \emph{Day-Night Classification} \parencite{DayNight}. eventhoug the deep learning approach is the most accurate for this specifique task, it add more computation cost wich will slow down the algorithm. So, we used the image basic statistiques to solve this issue.Figure \ref{fig:Fedwhcudlc} shows face/eyes detection with haarcascades under non-ideal lighting conditions.


\begin{figure}[!h]
\centering
  \subcaptionbox{}{\includegraphics[width = .2\textwidth]{f1}}\quad
  \subcaptionbox{}{\includegraphics[width = .2\textwidth]{f4}}\quad
  \subcaptionbox{}{\includegraphics[width = .2\textwidth]{f3}}\quad
  \subcaptionbox{}{\includegraphics[width = .2\textwidth]{f2}}
  \caption{face/eyes detection with Haar cascades under diferent lighting conditions}
\label{fig:Fedwhcudlc}
\end{figure}


In images (a) and (b), the detection was performed for the face and both eyes, but in the image (c), only the face was detected successfully, both detectors failed in the image (d). For a first test we assumed that the image (b) is the minimum luminosity value for face/eye detection, so we calculated basic statistics (Mean , Variance , Energy) for each image in Figure \ref{fig:Fedwhcudlc} in grayscale color system.


\begin{table}[!h]
\centering

\begin{tabular}{ | c | c | c | c | c | } 
\hline
 & image (a) & image (b) & image (c) & image (d) \\
\hline
Mean     & 68.47 & 13.82 & 6.71 & 5.66 \\
\hline
Variance & 2615.06 & 428.27 & 86.71 & 54.06 \\
\hline
Energy   & 111.55 & 42.29 & 31.41 & 29.59 \\
\hline
\end{tabular}
\caption{Table shows basic statistics for gray scale images of Figure \ref{fig:Fedwhcudlc} under difrent lighting conditions}
\label{table:BasicStatisticsForGrayScaleImages}
\end{table}

we need a criteria to classify lighting conditions into (good/bad). According to Table above, the Mean and Energy show a difference between the images, but we cannot use them because the difference between the values of the image (b) (minimum) and the image (c) of the Mean or Energy is too small and can be easily affected by light, however, the Variance shows a large difference between the image (b) and the image (c). So we used the Variance as ctiteria for the classification of lighting conditions.\\


To find the optimum values in diffrent conditions which was done in terms of TP and FP rates with the help of python programming language wich allows us to see algorithm results with the possibility of changing parameters values in real-time.

\begin{table}[!h]
\centering

\begin{tabular}{ |c|c|c|c|c|c|c|  }
 \hline
 & \multicolumn{2}{|c|}{Good LC} & \multicolumn{2}{|c|}{Bad LC}& \multicolumn{2}{|c|}{All LC}\\
 \hline
 & SF & MNN & SF & MNN &MinSize&MaxSize\\
 \hline
 Face & 1.3 & 1 & 1.1 & 1 & (116,116) &(331,331) \\
 \hline
Eye & 2.2 & 3 & None & None & (44,44) &(44,44) \\
 \hline
\end{tabular}

\caption{Table shows optimum values for detector parameters under difrent lighting conditions}
\label{table:OptimumValuesParameters}
\end{table}

Due to the fail of eyes detection under bad lighting conditions, we propose a night vision system based on image processing technique called "Power Law Transformation" (Gamma Correction) with the help of the Variance. Gamma correction is a nonlinear adaptation applied to each and every pixel value. Generally linear methods like addition, subtraction, and multiplication are applied on all the pixels. Gamma correction is responsible for performing nonlinear methods on the pixels of the input image and thereby remodeling the saturation of the image \parencite{THARSANEE2021117}. To inhance the quality of the image we must loop over all pixels of the image applying the power transformation equation:

 $$I'(i,j) = C \quad * \quad I(i,j)^{\quad \lambda}$$

Where C and $\lambda$ are positive constants and I is the intensity value of the
pixel in the input image at (i, j), and I' is the output intensity value. In most cases C = 1. if $\lambda$ = 1 then the mapping is linear and the output (I') image is the same as the input image. When $\lambda$ $<$ 1, the output image appears lighter than the input image. When $\lambda$ is bigger than 1, The effect is opposite, where the output image appears darker than the input. See  Figure\ref{fig:lambdaeffect} 

\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{lambda}
\caption{the effect of diffrent values of $\lambda$ for C = 1.}
\label{fig:lambdaeffect}
\end{figure}

Another approach \parencite{GPC} of the gamma correction is used in the application phase. First, our image must be scaled from the range [0 , 255] to [0.0 , 1.0]. Then we calculate the output image I' by applying the following equation:

 $$I'(i,j) =\quad I(i,j)^{\frac{1}{\gamma}}$$
 

 In this case, the output image will be lighter when $\gamma$ is $>$ 1 . And $\gamma$ values $<$ 1 will make the  output image appears darker than the input. See Figure \ref{fig:gammac}

 
\begin{figure}[!h]
\centering
  \includegraphics[width=.8\textwidth]{gammac}
  \caption{Power Law Transformation effect}
\label{fig:gammac}
\end{figure}


\newpage
  So based on the variance of a given frame in grayscale system we can check if it presents a good lighting conditions for face/eyes detection or we need to correct the luminance of the frame with the Power Law Transformation.
 
\begin{figure}[!h]
\centering
\includegraphics[width=.7\textwidth]{nightvision}
\caption{Flowchart illustrates the proposed night vision system based on variance and gamma correction}
\label{fig:nightvisionsystem}
\end{figure}


\begin{algorithm}[!h]
  \caption{Algorithm used in the proposed night vision system based on variance and gamma correction}
  \begin{itemize}
  	\item Foreach frame from video input :	
  	\begin{enumerate}	
		\item Change color space from RGB to Gray
		\item Variance calculation : 
$$ Variance  =  \frac{1}{XY}  \sum_{i=1}^{X}  \sum_{j}^{Y}  | I(i,j) - Mean(I)|^2  $$
		
		\item If ( Variance $<$ th ) :
			  \begin{itemize}
			  
				\item Change value of each pixel in the frame I by:
				 $$I(i,j) =\quad I'(i,j) =\quad I(i,j)^{\frac{1}{\gamma}}$$
				
			  \end{itemize}
			  		
  	\end{enumerate}
  \end{itemize}
\end{algorithm}

Using Variance threshold = 350 and a $\lambda$ value = 0.22, and the optimum detector parameters under a bad lighting conditions we got the following result:

\begin{figure}[!h]
\centering
  \subcaptionbox{}{\includegraphics[width = .4\textwidth]{dnvbefor}}\quad
  \subcaptionbox{}{\includegraphics[width = .4\textwidth]{dnvafter}}\quad
  \caption{Face / eye detection under diffrent lighting conditions}
\label{fig:nvbeforafter}
\end{figure}


For improoving the algorithm in terms of speed and calculations complexity, we decide to run the eye detector only if we find a face. And make it search in the ROI which is in this case the face region.

Finally, to invoke alarms we devided the driver state into four possible situations and enimurate these states to make the algorithm returns a number and based on that number we can invoke alarms. See Figure \ref{fig:lastflowchart}

\begin{figure}[h]
\centering
\includegraphics[width=.9\textwidth]{lastflowchart}
\caption{Flowchart illustrates the proposed driver monitoring algorithm based on Haar cascades}
\label{fig:lastflowchart}
\end{figure}
 

- 0 : the driver is awake and focusing on the road 

- 1 : the driver is showing a sign of drowsiness 

- 2 : the driver is drowsy 

- 3 : the driver is distracted 






\newpage
\section{method 2 : Facial key landmarks}
\emph{Face landmark detection is a computer vision task where we want to detect and track keypoints from a human face. This task applies to many problems.For example, we can use the keypoints for detecting a human’s head pose position and rotation. With that, we can track whether a driver is paying attention or not. In this section we use facial key landmarks to detect eye blink in real-time then decide wether the driver is awake or drowsy.}

Many methods have been proposed to automatically detect eye blinks in a video sequence. Several methods are based on a motion estimation in the eye region. Typically, the face and eyes are detected by a Viola-Jones type detector. Next, motion in the eye area is estimated from optical flow, by sparse tracking \parencite{Soukupov2016RealTimeEB}. A major drawback of the previous approaches is that they usually very sensetive for image resolution, illumination, motion dynamics, etc.

Thankfully, we don’t have to understand the concepts of face landmark detection in detail.However, robust real-time facial landmark detectors that capture most of the characteristic points on a human face image, including eye corners and eyelids, are available, we can use the prebuilt library like dlib, OpenCV, and mediapipe.These modern landmark detectors are trained on “in-the-wild datasets” like "Helen", "300W", and "LFPW" datasets, and they are thus robust to varying illumination, various facial expressions, and moderate non-frontal head rotations. An average error of the landmark localization of a state-of-the-art detector is usually below five percent of the inter-ocular distance. Recent methods run even significantly super real-time.

In this section we use dlib pretrained model. This model is based on ensemble regression trees because the model will predict continuous numbers. 











