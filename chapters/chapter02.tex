\chapter{Basic Concepts}


\emph{
In this chapter, we will simply introduce some basic concepts, methods, and mathematical background that we use in this thesis. We also provide symbols, image notations, and the equations that will be consistently used in the following chapters.
}


\section{Digital Image Processing Basics}
\emph{Processing digital images using a digital computer is known as digital image processing. We may alternatively call it the application of computer algorithms to improve an image or extract relevant information. \parencite{DIPGeeks}.}\\


In Digital Image Processing, signals captured from the physical world need to be translated into digital form by Digitization\footnote{Digitization: is the process of converting information into a digital (i.e. computer-readable) format.} Process. In order to become suitable for digital processing.

An image is defined as a two-dimensional function, I(x,y), where x and y are spatial coordinates, and the amplitude of I at any pair of coordinates (x,y) is called the intensity of that image at that point. An image must be digitized both spatially and in amplitude \parencite{DIPGeeks}. This digitization process involves two main processes \emph{Sampling}, and \emph{Quantization} \parencite{DIPMeduim}.

\subsection{Sampling}
In digital image processing, Sampling is the reduction of a continuous-time signal to a discrete-time signal. Since an analog image is continuous not just in its co-ordinates (x-axis), but also in its amplitude (y-axis), so the part that deals with the digitizing of co-ordinates is known as sampling \parencite{DIPViva}, see Figure \ref{fig:Sampling}.

\subsection{Quantization}
The process of transferring input values from a big set to output values in a smaller set, usually with a finite number of members, is known as quantization. Quantization is the opposite of sampling, It is done on the y-axis \parencite{DIPViva}, see Figure \ref{fig:Quantization}.

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{Sampling}  
  \caption{Sampling}
  \label{fig:Sampling}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{Quantization}  
  \caption{Quantization}
  \label{fig:Quantization}
\end{subfigure}
\caption{Sampling and Quantization}
\label{fig:SamplingAndQuantization}
\end{figure}

A digital image is typically composed of picture elements (pixels) located at the intersection of each row "\textbf{I}" and column "\textbf{J}" in each "\textbf{N}" color channels \parencite{MinakshiKumar}.
Digital images are stored in the form of a matrix of numbers where these numbers represent the intensity of each pixel, the range of these numbers (pixel values) is relative to the \textbf{Bit depth}\footnote{The bit depth "\textbf{k}" is the number of bits per pixel, the grey scale of an image is equal to $2^k$}, in general images are stored in 8 byte which means $2^8 = 256$ possible values.

\begin{figure}[!h]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{number8}  
  \caption{Matrix of pixels}
  \label{fig:MatrixOfPixels}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{matrix8}  
  \caption{pixels values}
  \label{fig:PixelValues}
\end{subfigure}
\caption{Gray scale image of Handwritten digit}
\label{fig:GrayScaleImageOfHandwrittenDigit}
\end{figure}

\section{Color Models}


Generally, images that are captured by camera sensors are color images, by default they use RGB color model \parencite{RgbAndCameras}, and because in the following chapters we mainly work on grayscale images like in Figure \ref{fig:GrayScaleImageOfHandwrittenDigit} we need to do a color space conversion. In this section, we will introduce briefly some basic concepts related to color models used in digital image processing and color space conversion


\subsection{RGB to Grayscale}
In the RGB color model, each color in the image is obtained by superimposing three colors, i.e., red, green, and blue. In this model, each pixel \textbf{P} in the image can be represented by $R(x_p , y_p )$, $G(x_p , y_p )$, $B(x_p , y_p )$. When $R(x_p , y_p )$ = $G(x_p , y_p )$ = $B(x_p , y_p )$ the given image becomes a grayscale image although it still has 3 color channels.\\

The goal is to convert the 3d \footnote{3d means 3 dimensional image (width, height, depth:'number of color channels')} RGB image to a 2d\footnote{2d means 2 dimensional (width, height)  } Grayscale image because smaller data enables developers to do more complex operations in a shorter time, there are a number of commonly used methods to convert an RGB image to a grayscale image such as the average method and weighted method \parencite{RGBTOGRAY}.\\

\subsubsection{Average method}
The Average method takes the average value of R, G, and B as the grayscale value as follows :

$$Grayscale =  \frac{ R + G + B }{3} $$ 

The average method is simple but doesn’t work as well as expected. The reason is that human eyeballs react differently to RGB. Eyes are most sensitive to green light, less sensitive to red light, and the least sensitive to blue light. Therefore, the three colors should have different weights in the distribution. That brings us to the weighted method.


\subsubsection{The Weighted Method}

The weighted method, also called the luminosity method, weighs red, green, and blue according to their wavelengths. The improved formula is as follows:

$$Grayscale = 0.299R + 0.587G + 0.114B$$

\section{Image Values and Statistics}
\emph{Considering \emph{\textbf{I}} a grayscale image, and X, Y are numbers of rows, columns respectively.}

 
\textbf{Mean} : 
$$ mean  =  \frac{1}{XY}  \sum_{i=1}^{X}  \sum_{j}^{Y}  I(i,j) $$

\textbf{Variance}: 
$$ variance  =  \frac{1}{XY}  \sum_{i=1}^{X}  \sum_{j}^{Y}  | I(i,j) - mean|^2  $$


\textbf{Energy}: 
$$ energy  =  \frac{1}{XY}  \sum_{i=1}^{X}  \sum_{j}^{Y}  | I(i,j)|^2  $$

\section{Classification}

Classification is a process that uses a set of features or parameters to recognize an object. In this thesis, we use supervised classification techniques, which means that an expert defines the classes of objects (e.g., face, eye, vehicles), and also provides a set of sample objects for a given class which is called the training set. Regardless of the chosen classification technique (e.g., neural networks, decision trees, or nearest neighbor rule), we have two phases to construct a classifier: a training phase and an application phase.\\

Based on the provided training dataset, a classifier learns to use which set of features, parameters, and weights to be combined together in order to distinguish objects from non-objects. In the application phase, the classifier uses the previously taught features and weights to find comparable objects in an unknown query image, based on what it has learned from the training set.\\

The detection efficiency may be used to evaluate a classifier's performance regardless of the classification approach used. Let \textbf{TP} stand for the number of true-positives or hits, which occur when a classifier successfully detects the objects. Also, let \textbf{FP} be the number of false-positives or miss, when a classifier wrongly detects a non-object as an object. Similarly, we can define true-negatives \textbf{(TN)} and false-negatives \textbf{(FN)} to describe the correct classification of non-objects and the missing objects, respectively. Although we always can measure (count) the number of \textbf{FN}, we can not simply define the number of \textbf{TN} for an application such as vehicle detection in a road scene.\\ 

This is due to the fact that the backdrop of an image is virtually uncountable. Therefore, for performance evaluation of a classifier, we mainly rely on evaluations using TP, FP, and FN.

We define the precision-rate (PR), recall-rate (RR), and accuracy (ACC) as follows \parencite{Classification} :

\begin{center}
$$ PR = \frac{TP}{TP + FP} $$ \\  
$$ RR = \frac{TP}{TP + FN} $$ \\
$$ ACC = \frac{TP + TN}{TP + FN + TN +FP}  $$
\end{center}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fpfntptn}
\caption{Illustration of FP, FN, TP, and TN}
\label{fig:Ifpfntptn}
\end{figure}


Where PR is the ratio of true-positives to all detections by the classifier, RR is the ratio of true-positives to the total number of actually existing objects (i.e. the ground truth objects) in the analyzed frames and ACC is defined as the number of classifications a model correctly predicts divided by the total number of predictions made. It's a way of assessing the performance of a model.

\section{Integral Image}

For a given image I, the integral image $I_{int}$ , which was first used by Viola and Jones in computer vision \parencite{PaulViola}, is the summation of all pixel values in the image, or in a window (sub-image).\\


Rectangle features may be calculated quickly utilizing an intermediate picture representation known as the integral image. The integral image at location $(x,y)$ contains the sum of the pixels above and to the left of $(x,y)$ , inclusive:

$$ I_{int}(x,y)= \sum_{x' <= x,y' <= y}^{} i(x' , y' )$$


\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{integral_image}
\caption{Calculation of integral values and integral image}
\label{fig:IntegralImage}
\end{figure}

Taking the figure \ref{fig:IntegralImage} as an example, The sum of the pixels within rectangle \textbf{D} can be computed with four array references. The value of the integral image at location 1 is the sum of the pixels in rectangle \textbf{A}. The value at location 2 is \textbf{A + B}, at location 3 is \textbf{A + C},and at location 4 is \textbf{A + B + C + D}. The sum within \textbf{D} can be computed as \textbf{$4 + 1 - (2 + 3)$}.

Having the integral values of each pixel calculated and saved in a data structure
array, we can calculate the integral image of any image or sub-image just by applying one addition and two subtraction operations. This is very fast and cost-efficient
for real-time feature-based classification algorithms, with a computational complexity of $O(N_{cols} N_{rows})$.


\section{Haar Feature-based Classifiers}
\emph{Inspired by Haar-wavelets, Viola and Jones \parencite{PaulViola} introduced the idea of Haar-like features with square-shaped adjacent black and white patterns for the first time see Figure \ref{fig:HaarLikeFeatures}, in the area of face detection.}

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{haar_examples}
\caption{Common types of Haar-like features}
\label{fig:HaarLikeFeatures}
\end{figure}

The common types of Haar-like features are \emph{Line Features},\emph{Edge features}, and \emph{Diagonal features}. They are just like the convolutional kernel. Each feature is a single value derived by subtracting the sum of pixels under the white rectangle from the sum of pixels under the black rectangle; the integral image is used to simplify calculations. These features are very important in the context of face detection because they can detect a face based on the face's properties see Figure \ref{fig:HaarLikeFeaturesDetection}.

\begin{figure}[h]
\centering
\includegraphics[width=.6\textwidth]{face_props}
\caption{Example of Haar-feature matches in eyes}
\label{fig:HaarLikeFeaturesDetection}
\end{figure}

\newpage Consider the Figure \ref{fig:HaarLikeFeaturesDetection}. The top row shows two good features. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks. The second feature selected relies on the property that the eyes are darker than the bridge of the nose.\\

A \emph{strong classifier} comprises a series of weak classifiers (normally more than 10), and a weak classifier itself includes a set of a few Haar features (normally 2 to 5).

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{classifiers_haar}
\caption{A cascade of weak-classifiers}
\label{fig:CascadeOfWeakClassifiers}
\end{figure}

Figure \ref{fig:CascadeOfWeakClassifiers} visualizes a cascade of weak classifiers, that all together make a strong
classifier. The classifier starts with the first weak classifier by evaluating the region of interest (ROI). It proceeds to the second stage (second weak classifier) if all the Haar-features in the first weak classifier match with the ROI, and so on, until reaching the final stage; otherwise the search-region under the sliding window will be rejected as non-object. Then the sliding window moves to the next neighbour ROI. If all Haar-features in all weak classifiers successfully match the ROI, the region will be designated as a discovered item with a bonding box.  \\

The training step, which might be done using a machine learning approach like the AdaBoost algorithm \parencite{Adaboost}, consists of picking acceptable Haar features for each weak classifier and subsequently for the whole strong classifier.

\newpage
\subsection{AdaBoost algorithm}
AdaBoost is an approach to machine learning based on the idea of creating a highly accurate prediction rule by combining many relatively weak and inaccurate rules. The AdaBoost algorithm of Freund and Schapire was the first practical boosting algorithm, and remains one of the most widely used and studied, with applications in numerous fields \parencite{Adaboost}.}

\begin{algorithm}[h]
  \caption{The AdaBoost algorithm for classifier learning.}
  \begin{itemize}
  	\item 
  	Given example images $(x_1,y_1),...,(x_n,y_n)$ where $y_i = 0,1$ for negative and positive examples respectively. 

    \item
    Initialize the weights $w_{1,i} = \frac{1}{2m}, \frac{1}{2l}$ for $y_i = 0,1 $ respectively, where $m$ and $l$ are the number of negatives and positives respectively. 

    \item
    For $t = 1$ to $T$:
    
    \begin{enumerate}
	  \item
	  Normalize the weights,
	  $$w_{t,i} \leftarrow \frac{ w_{t,i} } { \sum_{j=1}^{n} w_{t,j}} $$    
	  so that $w_t$ is a probability distribution.
	
	 \item
	  For each feature, $j$, train a classifier $h_j$ which is restricted to using a single feature. The error is evaluated with respect to 
	  $$w_t , \epsilon_j = \sum_{i} w_i |h_j(x_i)-y_i|$$
	  
	  \item
	  Choose the classifier, $h_t$,with the lowest error $\epsilon_t$.
	  
	  \item
	  Update the weights:
	  $$w_{t+1,i} = w_{t,i}\beta_{t}^{1-e_i}$$
	  
	  where $e_i = 0$ if example $x_i$ is classified coreectly, $e_i = 1$ otherwise,and $\beta_t = \frac{\epsilon_t}{1-\epsilon_t}$ 
      
    \end{enumerate}
	
   \item
   The final strong classifier is:

   $$ h(x) = \{_{o \quad\quad otherwise}^{1 \quad\quad \sum_{t=1}^{T} \alpha_t h_t(x) \geq \frac{1}{2} \sum_{t=1}^{T} \alpha_t } \} $$
  
   where $\alpha_t = \log \frac{1}{\beta_t}$  
  \end{itemize}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%% khansaae
\subsection{Support-vector machine (SVM)}

Support vector machines (SVM) are supervised machine learning models with associated learning algorithms that analyze data for classification and regression analysis.
The goal of the support vector machine technique is to identify a hyperplane in an "N" dimensional space (where N is the number of features) that categorizes the data points.

 There are two types of SVM "linear" and "non-linear", Linear SVM can be used when the data is perfectly linearly separable, and non-linear SVM is used when the data is not linearly separable see Figure \ref{fig:linear}
 
 
\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{linearvsnonlinear}
\caption{Linear vs non-linear 2d points}
\label{fig:linear}
\end{figure}



\subsection{Histogram of oriented gradients (HOG)}	

The Histogram of Oriented Gradients (HOG) is a feature descriptor that is used in image processing to recognize objects. A feature descriptor is an image or image patch representation that simplifies the picture by extracting valuable information. It is better than any other edge descriptor because it computes features using both the magnitude and angle of the gradient \parencite{HOGfeature2}.


According to Navneet Dalal and Bill Triggs in their paper "Histograms of Oriented Gradients for Human Detection", The first step is to resize the input image into an image of 128x64 pixels, this step is not required but desirable as it is suggested by the authors for better results. The second step is to compute the image gradient in both the x and y direction (Gx , Gy) for all pixels within the image followed by calculation of the magnitude of the gradient as (G) and the direction of the gradient as ($\theta$) using the following equations :

$$ G_x  =\quad I( i  ,  j + 1 ) \quad - \quad I( i  ,  j + 1 )  $$

$$ G_y  =\quad I( i - 1  ,  j ) \quad - \quad I( i + 1  ,  j )  $$


$$ G  =\quad \sqrt[2]{\quad G_{x}^{2} + G_{y}^{2}  \quad}  $$


$$ \theta =\quad |  \tan^{-1} (\frac{G_y}{G_x}) |  $$


After that, the input image is divided into 8 x 8 cell blocks, and a histogram of gradients is calculated for each block followed by vector normalizations.To calculate the final feature vector for the entire image patch, the 36×1 vectors are concatenated into one giant vector. However, a complete pipeline for object detection using HOG and SVM can be represented as follows \parencite{HOGfeature}:


\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{HOGandSVM}
\caption{Object detection using HOG and SVM pipeline}
\label{fig:lool}
\end{figure}

                  


 
